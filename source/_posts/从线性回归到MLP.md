---
title: ä»çº¿æ€§å›å½’åˆ°MLP
date: 2025-01-20 00:21:35
tags:
  - machine learning
  - python
categories: å­¦ä¹ ç¬”è®°
---

# çº¿æ€§æ¨¡å‹
## æ¨¡å‹æ­å»º
çº¿æ€§æ¨¡å‹æ˜¯æœ€ç®€å•çš„æ•°å­¦æ¨¡å‹ï¼Œä»–çš„æ•°å­¦å…¬å¼æ˜¯ä¸‹é¢è¿™ä¸ª:
$$f(\vec{x})=\sum_{i=1}^{n}w_{i}x_{i}+b$$
æƒé‡ï¼‹å˜é‡æ±‚å’Œæœ€ååŠ ä¸Šå¸¸é‡ã€‚

æˆ‘ä»¬ä»¥ä¸€ä¸ªåœ°åŒºæˆ¿ä»·çš„å˜åŠ¨é¢„æµ‹ä¸ºä¾‹ï¼Œä»–çš„å‚æ•°å¯èƒ½æœ‰æˆæœ¬ï¼Œåˆ©æ¶¦ï¼Œäººæ•°ï¼Œæ”¶å…¥ç­‰ç­‰å¯èƒ½ä¸é¢„æµ‹å€¼æœ‰å…³ç³»çš„å‚æ•°ï¼Œè¿™äº›ç»Ÿè®¡å°±æ˜¯åœ¨åšç‰¹å¾å·¥ç¨‹ã€‚

æˆ‘ä»¬æ”¶é›†çš„æ•°æ®å¯ä»¥è¡¨ç¤ºä¸º
$$X=[\vec{X}_{1},\vec{X}_{2},\vec{X}_{3},\ldots\vec{X}_{N}]^{T}$$
$$Y=[\vec{Y}_{1},\vec{Y}_{2},\vec{Y}_{3},\ldots\vec{Y}_{N}]^{T}$$
å…¶ä¸­Xæ˜¯ç‰¹å¾è®¡ç®—ï¼ŒYæ˜¯æˆ¿ä»·ï¼ŒNæ˜¯æ•°æ®æ•°ç›®ã€‚

æ¥ä¸‹æ¥æˆ‘ä»¬å°±æ˜¯è®­ç»ƒï¼Œé€šè¿‡è°ƒæ•´å‚æ•°ä½¿å¾—æ ¹æ® $x$ è®¡ç®—å‡ºæ¥çš„ $f(\vec{x})$ ä¸çœŸå®çš„ $Y$ å°½å¯èƒ½çš„å·®è·å°ã€‚

è¿™é‡Œæœ‰ä¸ªéœ€è¦æ³¨æ„çš„åœ°æ–¹ï¼Œæˆ‘ä»¬çŸ¥é“æˆ¿ä»·çš„å€¼æ˜¯ä¸èƒ½å°äº0çš„ï¼Œè¿™æ—¶å€™æˆ‘ä»¬éœ€è¦å¯¹æ¨¡å‹è¿›è¡Œè½¬åŒ– (åœ¨æ·±åº¦å­¦ä¹ é‡Œå«æ¿€æ´»å‡½æ•°) ï¼š
$$f(\overrightarrow{x})=\max(0,\sum_{i=1}^{n}w_{i}x_{i}+b)$$
å…¶ä¸­è¿™ä¸ª $\max(x,y)$ åˆå« [ReLU å‡½æ•°](https://zhuanlan.zhihu.com/p/428448728)ï¼Œå½“ç„¶ä¹Ÿæœ‰åˆ«çš„å‡½æ•°ï¼Œæ¯”å¦‚ç»“æœåªåœ¨ 0~1 ä¹‹é—´çš„ [Sigmoid å‡½æ•°](https://zhuanlan.zhihu.com/p/424858561)ç­‰ç­‰ï¼Œè¿™é‡Œå°±ä¸å¤šåšä»‹ç»äº†

## æŸå¤±å‡½æ•°
æŸå¤±å‡½æ•°é€šå¸¸ç”¨äºè®¡ç®—æ¨¡å‹å¯¹å•ä¸ªæ ·æœ¬é¢„æµ‹ç»“æœçš„è¯¯å·®ï¼Œæˆ¿ä»·ä¾‹å­ä¸­çš„æŸå¤±å‡½æ•°å¯ä»¥è¡¨ç¤ºä¸º: 
$$C(\vec{w},b,X,Y)=\frac{1}{N}\sum_{i=1}^{N}(y_{i}-\max(0,f(\vec{x}_{i},\vec{w},b)))^{2}$$
æˆ‘ä»¬é€šè¿‡è®¡ç®—æ¨¡å‹ä½¿ç”¨çš„ ReLU å‡½æ•°è®¡ç®—å‡ºçš„ç»“æœï¼Œä¸çœŸå®å€¼ç›¸å‡çš„å¹³æ–¹æ±‚å¹³å‡æ•°ï¼Œè®©ä»–å°½å¯èƒ½çš„å°ï¼Œé€šè¿‡è¿™ç§å½¢å¼ï¼Œè®²ä¸€ä¸ªä¼˜åŒ–é—®é¢˜ï¼Œè½¬æ¢ä¸ºäº†æ•°å­¦é—®é¢˜ã€‚

ç°åœ¨ï¼Œæˆ‘ä»¬çš„ç›®æ ‡å°±å˜æˆäº†ï¼Œè°ƒæ•´ $\vec{w}$ å’Œ $b$ ï¼Œè®©æŸå¤±å‡½æ•°ç»“æœå°½å¯èƒ½å°ã€‚

è¿™é‡Œä¼šå¼•å‡ºä¸€ä¸ªç»å…¸é—®é¢˜ï¼Œè¿‡æ‹Ÿåˆï¼Œå› ä¸ºå­¦ä¹ å®é™…ä¸Šä»–å¹¶ä¸æ˜¯ä¸€ä¸ªçœŸå®çš„æ•°å­¦ä¼˜åŒ–é—®é¢˜ï¼Œå¦‚æœè¿‡æ‹Ÿåˆï¼Œä¼šå¯¼è‡´æ³›åŒ–èƒ½åŠ›å˜å¼±ï¼Œè¿™æ˜¯éœ€è¦æ³¨æ„çš„ï¼Œè¿™ä¹Ÿæ˜¯æœºå™¨å­¦ä¹ å’Œæ•°å­¦ä¼˜åŒ–çš„é‡è¦åŒºåˆ«ã€‚

![Cost function](/image/machine-learning/cost-funtion.png)

## å¦‚ä½•ä¼˜åŒ–
è¿™é‡Œæ¨è Stephen Boyd çš„ [Convex Optimization](https://www.youtube.com/watch?v=kV1ru-Inzl4)ï¼Œè¿™é‡Œåªåšä¸€äº›ç®€å•çš„ä»‹ç»ï¼Œä¸€èˆ¬ä¼˜åŒ–åˆ†ä¸ºä¸¤ç§ï¼Œä¸€ç§æ˜¯æ¢¯åº¦æ€§ä¼˜åŒ–ï¼Œå¦ä¸€ç§æ˜¯å¯¹å¶æ€§ä¼˜åŒ–ï¼ˆä¸€èˆ¬æ˜¯è½¬æˆæ›´å¥½ä¼˜åŒ–çš„é—®é¢˜å»ä¼˜åŒ–ï¼‰ã€‚è¿™é‡Œæˆ‘ä»¬ä¸»è¦ä»‹ç»åœ¨æˆ‘ä»¬è¿™ä¸ªæ¨¡å‹ä¸­ç”¨åˆ°çš„æ¢¯åº¦æ€§ä¸‹é™ä¼˜åŒ–ã€‚

æ¢¯åº¦å°±æ˜¯è¿™ä¸ªå‡½æ•°çš„ä¸åŒå˜é‡å€’æ•°ç»„æˆçš„å˜é‡ã€‚

$$[\frac{\partial f}{\partial x_{1}},\frac{\partial f}{\partial x_{2}}\cdots\frac{\partial f}{\partial x_{n}}]$$

æ¢¯åº¦ä¸‹é™ä½ å¯ä»¥ç†è§£ä¸ºï¼Œæˆ‘ä»¬æœ‰ä¸ªå‡½æ•°ï¼Œæˆ‘ä»¬é€šè¿‡å¯»æ‰¾ä»–å˜é‡ä¸­å„ä¸ªæ–¹å‘ä¸Šä¸‹é™æœ€å¿«çš„æ–¹å‘å»ç§»åŠ¨ï¼Œä»è€Œæ‹¿åˆ°ä»–çš„å…¨å±€æœ€ä½ç‚¹ã€‚

![optimization](/image/machine-learning/optimization.jpg)

æ¯”å¦‚å·¦å›¾ï¼Œå½“å¯¼æ•°ä¸º0æ—¶ï¼Œå¯ä»¥æ‹¿åˆ°ä»–çš„æœ€å°å€¼ï¼Œè¿™ä¸ªä¼˜åŒ–çš„å‰ææ˜¯è¿™ä¸ªå‡½æ•°æ˜¯å‡¸å‡½æ•°ã€‚

å¦‚æœå‡ºç°å³å›¾è¿™ç§ä¸æ˜¯å‡¸å‡½æ•°ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦é€šè¿‡ä¿®æ”¹æ­¥é•¿ï¼ˆéšæœºæ‰°åŠ¨ï¼‰ï¼Œä»è€Œè·³è¿‡å±€éƒ¨æœ€ä¼˜è§£ï¼Œæ‰¾åˆ°å…¨å±€æœ€ä¼˜è§£ã€‚éå‡¸æœ€ä¼˜è§£é—®é¢˜åœ¨æ•°å­¦ä¸Šæ˜¯ä¸€ä¸ªæ¯”è¾ƒå¤æ‚çš„äº‹æƒ…ï¼Œæˆ‘ä»¬å¯ä»¥æ‹›ä¸€ä¸ªè¿‘ä¼¼æœ€ä¼˜è§£æ¥è§£å†³ã€‚

## é“¾å¼æ³•åˆ™
çŸ¥é“äº†ä¼˜åŒ–æ–¹æ³•åï¼Œæˆ‘ä»¬ä¸‹ä¸€æ­¥å°±æ˜¯è¦æ±‚æ¢¯åº¦ï¼Œä¹Ÿå°±æ˜¯æ±‚åå¯¼ã€‚è¿™é‡Œä»‹ç»ä¸€ä¸‹å¤§å­¦é«˜æ•°é‡Œçš„é“¾å¼æ³•åˆ™ï¼Œä»–å¯ä»¥æŠŠå¤æ‚çš„æ±‚å¯¼åŒ–ç®€ã€‚

$$y(x)=f(g(x))$$

$$y^{\prime}(x)=f^{\prime}(g(x))\cdot g^{\prime}(x)$$

è¿™é‡Œå¸®å¤§å®¶å›å¿†ä¸€ä¸‹è¿™ä¸ªè¯æ˜
$$y^{\prime}(x)=\lim_{\Delta x\to0}\frac{f(g(x+\Delta x))-f(g(x))}{\Delta x}$$
$$=\lim_{\Delta x\to0}\frac{f(g(x)+\Delta y)-f(g(x))}{\Delta x}$$
$$=\lim_{\Delta x\to0}\frac{f(g(x)+\Delta g)-f(g(x))}{\Delta g}\cdot\frac{\Delta g}{\Delta x}$$
$$=f^{\prime}(g(x))\cdot g^{\prime}(x)$$
$$let:\Delta g=g(x+\Delta x)-g(x))$$
$$g(x+\Delta x)=g(x)+\Delta g$$

æœ‰äº†è¿™ä¸ªåï¼Œæˆ‘ä»¬å¯ä»¥çœ‹é“¾å¼æ³•åˆ™åœ¨å‘é‡ä¸Šçš„æ¨å¹¿ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸ªæ–¹ç¨‹ç»„:
$$\vec{f}(\vec{x})\Rightarrow\begin{cases}
f_{1}(\overrightarrow{x})=ax_{1}+bx_{2}^{2}+cx_{3}^{3} \\
f_{2}(\overrightarrow{x})=dx_{1}+\sin x_{2} \\
f_{2}(\overrightarrow{x})=\cdots x_{1}+\cdots x_{2} & 
\end{cases}$$

ä»–çš„å…¶ä¸­ä¸€ä¸ªå‡½æ•°çš„æ¢¯åº¦å°±æ˜¯:
$$[\frac{\partial f_{1}}{\partial x_{1}},\frac{\partial f_{1}}{\partial x_{1}},\frac{\partial f_{1}}{\partial x_{3}}]$$

æ•´ä½“æ¢¯åº¦ Jacobi çŸ©é˜µå°±æ˜¯:
$$[
\begin{array}
{c}\frac{\partial f_{1}}{\partial x_{1}},\frac{\partial f_{1}}{\partial x_{2}},\frac{\partial f_{1}}{\partial x_{3}} \\
\frac{\partial f_{2}}{\partial x_{1}},\frac{\partial f_{2}}{\partial x_{2}},\frac{\partial f_{2}}{\partial x_{3}} \\
\frac{\partial f_{3}}{\partial x_{1}},\frac{\partial f_{3}}{\partial x_{1}},\frac{\partial f_{3}}{\partial x_{3}}
\end{array}]$$
ä¸­é—´çš„è®¡ç®—ï¼Œå¯ä»¥å‚è€ƒ [The Matrix Calculus You Need For Deep Learning](https://explained.ai/matrix-calculus/) è¿™æœ¬å°ä¹¦ï¼Œé‡Œé¢ä»‹ç»äº†æ·±åº¦å­¦ä¹ éœ€è¦çš„çŸ©é˜µè®¡ç®—çš„çŸ¥è¯†ã€‚ç»“æœå°±æ˜¯ï¼š
$$\frac{\partial \vec{f}(\overrightarrow{g}(\overrightarrow{x}))}{\partial \overrightarrow{x}}=\frac{\partial \overrightarrow{f}}{\partial \overrightarrow{g}}\cdot\frac{\partial \overrightarrow{g}}{\partial \overrightarrow{x}}$$
å…¶ä¸­
$$f(\overrightarrow{x})\Rightarrow
\begin{cases}
g_{1}(\overrightarrow{x}) \\
g_{2}(g_{1}(\overrightarrow{x})) \\
g_{3}(g_{2}(\overrightarrow{x})) & 
\end{cases}$$
å¯ä»¥å¾—åˆ°
$$\frac{\partial f(\overrightarrow{x})}{\partial\overrightarrow{x}}=\frac{\partial g_{3}}{\partial g_{2}}\cdot\frac{\partial g_{2}}{\partial g_{1}}\cdot\frac{\partial g_{1}}{\partial\overrightarrow{x}}$$

## è§£å†³é—®é¢˜
æœ‰äº†ä»¥ä¸ŠçŸ¥è¯†ï¼Œæˆ‘ä»¬å¯ä»¥å›åˆ°æˆ‘ä»¬ä¹‹å‰çš„æˆ¿ä»·é—®é¢˜ä¸Šæ¥è§£å†³å®ƒäº†ã€‚æˆ‘ä»¬çš„éœ€æ±‚æ˜¯å°†æŸå¤±å‡½æ•°æœ€å°åŒ–ï¼š
$$C(\vec{w},b,X,Y)=\frac{1}{N}\sum_{i=1}^{N}(y_{i}-\max(0,f(\vec{x}_{i},\vec{w},b)))^{2}$$
æˆ‘ä»¬å¯ä»¥å°†æŸå¤±å‡½æ•°è½¬åŒ–ä¸º
$$C(\vec{w},b,X,Y)\Rightarrow\begin{cases}
u(\overrightarrow{w},\overrightarrow{x},b) & =\max(0,\overrightarrow{w}\overrightarrow{x}_{i}+b) \\
v(y,u) & =y-u \\
c(v) & =\frac{1}{N}\sum_{i=1}^{N}v^{2} & 
\end{cases}$$
æ¥ä¸‹æ¥æˆ‘ä»¬ä¸€å±‚ä¸€å±‚æ±‚åå¯¼
$$\frac{\partial u}{\partial\overrightarrow{w}}=
\begin{cases}
\overrightarrow{0}^{T} & \overrightarrow{w}\overrightarrow{x_{i}}+b\leq0 \\
\overrightarrow{x} & \overrightarrow{w}\overrightarrow{x_{i}}+b>0 & 
\end{cases}$$
$$\frac{\partial v}{\partial\vec{w}}=\frac{\partial v}{\partial u}\cdot\frac{\partial u}{\partial\vec{w}}=(0-1)\frac{\partial u}{\partial\vec{w}}=-\frac{\partial u}{\partial\vec{w}}$$
$$\frac{\partial c}{\partial\vec{w}}=\frac{\partial}{\partial\vec{w}}\frac{1}{N}\sum_{i=1}^{N}v^{2}=\frac{1}{N}\sum_{i=1}^{N}\frac{\partial}{\partial\vec{w}}v^{2}=\frac{1}{N}\sum_{i=1}^{N}2v\frac{\partial v}{\partial\vec{w}}$$

åˆ°äº†æœ€å¤–å±‚åï¼Œæˆ‘ä»¬å°±å¯ä»¥æŠŠå®ƒä»¬æ•´åˆèµ·æ¥äº†
$$\frac{\partial C}{\partial\vec{w}}=\frac{\partial}{\partial\vec{w}}\frac{1}{N}\sum_{i=1}^{N}v^{2}=\frac{1}{N}\sum_{i=1}^{N}\frac{\partial}{\partial\vec{w}}v^{2}=\frac{1}{N}\sum_{i=1}^{N}2v\frac{\partial v}{\partial\vec{w}}=\frac{1}{N}\sum_{i=1}^{N}
\begin{cases}
\vec{0}^{T},(\vec{w}\vec{x}_{i}+b\leq0) \\
-2v\vec{x}^{T}(\vec{w}\vec{x}_{i}+b>0) & 
\end{cases}=
\begin{cases}
\vec{0}^{T},(\vec{w}\vec{x}_{i}+b\leq0) \\
\frac{2}{N}\sum_{i=1}^{N}(\overrightarrow{w}\cdot\overrightarrow{x}_{i}+b-y_{i})x_{i}^{T} & 
\end{cases}=
\begin{cases}
\vec{0}^{T},(\vec{w}\vec{x}_{i}+b\leq0) \\
\frac{2}{N}\sum_{i=1}^{N}e_{i}x_{i}^{T} & 
\end{cases}$$

å…¶ä¸­ $e_{i} = \overrightarrow{w}\cdot\overrightarrow{x}_{i}+b-y_{i}$ æ˜¯è¯¯å·®å‡½æ•°

è¿™æ ·å°±å¯ä»¥å¾—åˆ°æˆ‘ä»¬çš„ç®—æ³•äº†ï¼š
- æˆ‘ä»¬å…ˆéšæœºå–ä¸€ä¸ªéšæœºæƒé‡ $\vec{w}_{0}^{T}$ ï¼ŒæŸ¥çœ‹ç»“æœï¼Œå¦‚æœæ‰“åˆ°å¯æ¥å—èŒƒå›´åˆ™ç»“æŸï¼Œå¦åˆ™ä¸‹ä¸€æ­¥
- è®¡ç®—æ¢¯åº¦ $\frac{2}{N}\sum_{i=1}^{N}e_{i}x_{i}^{T}$
- å¾—åˆ°æ–°æƒé‡ $\vec{w}_{1}^{T} = \vec{w}_{0}^{T} - \frac{2}{N}\sum_{i=1}^{N}e_{i}x_{i}^{T}\cdot\eta$

## Coding
```python
import numpy as np

class MyLinearRegression:
    # å­¦ä¹ ç‡ è¿­ä»£æ¬¡æ•° æ¿€æ´»å‡½æ•°
    def __init__(self, learning_rate=0.01, n_iterations=1000, activation=None):
        self.learning_rate = np.float64(learning_rate)
        self.n_iterations = n_iterations
        self.weights = None
        self.bias = None
        self.activation = activation
    
    def _relu(self, x):
        return np.maximum(0, x)

    def _relu_derivative(self, x):
        return (x > 0).astype(float)

    def _sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def _sigmoid_derivative(self, x):
        s = self._sigmoid(x)
        return s * (1 - s)
    
    def _apply_activation(self, x):
        if self.activation == 'relu':
            return self.relu(x)
        elif self.activation == 'sigmoid':
            return self.sigmoid(x)
        return x  # No activation

    def _apply_activation_derivative(self, x):
        if self.activation == 'relu':
            return self.relu_derivative(x)
        elif self.activation == 'sigmoid':
            return self.sigmoid_derivative(x)
        else:
            return 1 # Derivative is 1 when no activation

    def fix(self, X, y):
        n_samples, n_features = X.shape

        # åˆå§‹éšæœºæƒé‡
        self.weights = np.random.randn(n_features).astype(np.float64) * 0.01
        self.bias = np.float64(0)

        # é€€å‡ºé˜ˆå€¼ï¼Œè¿ç»­äº”æ¬¡å°äº 1e-5
        prev_loss = float('inf')
        patience = 5
        min_change = 1e-5
        patience_counter = 0

        for _ in range(self.n_iterations):
            linear_output = np.dot(X, self.weights) + self.bias
            y_predicted = self.apply_activation(linear_output)
            
            activation_derivative = self.apply_activation_derivative(linear_output)
            diff = y_predicted - y

            # w å’Œ b çš„æ¢¯åº¦
            dw = np.float64(1/n_samples) * np.dot(X.T, (diff * activation_derivative))
            db = np.float64(1/n_samples) * np.sum(diff * activation_derivative)

            # è®¾ç½®ä¸ªèŒƒå›´ï¼Œä¸è¦å˜åŒ–è¿‡å¤§
            clip_threshold = 2.0
            dw = np.clip(dw, -clip_threshold, clip_threshold)
            db = np.clip(db, -clip_threshold, clip_threshold)

            if not (np.isnan(dw).any() or np.isnan(db)):  
                self.weights -= self.learning_rate * dw
                self.bias -= self.learning_rate * db

            current_loss = np.mean(y_predicted - y) ** 2
            # å°äºé˜ˆå€¼å°±ç»“æŸ
            if abs(prev_loss - current_loss) < min_change:
                patience_counter += 1
                if patience_counter >= patience:
                    break
            else:
                patience_counter = 0
            prev_loss = current_loss
    
    def predict(self, X):
        linear_output = np.dot(X, self.weights) + self.bias
        return self.apply_activation(linear_output)

    
    def score(self, X, y):
        y_pred = self.predict(X)
        ss_total = np.sum((y - y.mean()) ** 2)
        ss_residual = np.sum((y - y_pred) ** 2)
        return 1 - (ss_residual / ss_total)
```

```python
# è®¡ç®—ç”¨
import numpy as np
# æ•°æ®åˆ†æç”¨
import pands as pd

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from LinearModel import MyLinearRegression

if __name__ == '__main__':
    # load data
    df = pd.read_csv('California_Houses.csv')

    # æ‹†åˆ†
    X = df.drop('Median_House_Value', axis=1)
    y = df.drop['Median_House_Value']

    # è®­ç»ƒé›†ï¼Œæµ‹è¯•é›†
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # å…ˆç”¨åˆ«äººçš„æ¨¡å‹è·‘ä¸‹
    model = LinearRegression()
    model.fit(X_train, y_train)
    print("R-squared score: ", model.score(X_test, y_test))

    myModel = MyLinearRegression(learning_rate=0.001, n_iterations=10000, activation='relu')
    myModel.fit(X_train, y_train)
    print("R-squared score: ", model.score(X_test, y_test))

    # å°è¯•ä¼˜åŒ–ä¸‹æ¨¡å‹
    # å…ˆçœ‹ä¸‹åˆ†å¸ƒå›¾
    # numerical_columns_columns_coltypes(include=[`float64',"int64']).columns
    # num_columns_len(numerical_columns)
    # num_rows = (num_columns // 3)+(1 if num_columns % 3 !=0 else 0)
    # num_cols = 3

    # plt.figure(figsize=(15, 6 * num_rows))
    # for i, column in enumerate(num_columns, 1):
    #     plt.subplot(num_rows, num_cols, i)
    #     sns.histplot(df[column], kde=True)
    #     plt.title(f'Distribution of {column}')
    # plt.tight_layout()
    # plt.show()


    df['Distance_to_BigCty'] = np.min(df['Distance_to_LA','Distance_to_SanDiego'])

    df['Bedroom_per_household'] = df['Households'] / df['Households']

    df['Median_House_Value'] = np.log(df['Median_House_Value'])
    df['Median_Income'] = np.log(df['Median_Income'])
    df['Distance_to_coast'] = np.log(df['Distance_to_coast'])
    df['Bedroom_per_household'] = np.log(df['Bedroom_per_household'])

    newX = df.drop('Median_House_Value', axis=1)
    new_y = df.drop['Median_House_Value']

    # è®­ç»ƒé›†ï¼Œæµ‹è¯•é›†
    new_X_train, new_X_test, new_y_train, new_y_test = train_test_split(newX, new_y, test_size=0.2, random_state=42)

    model = LinearRegression()
    model.fit(new_X_train, new_y_train)
    print("R-squared score: ", model.score(new_X_test, new_y_test))
```
<!-- 
# å†³ç­–æ ‘

å‡è®¾æˆ‘ä»¬æœ‰ä¸ªè´·æ¬¾æ˜¯å¦èƒ½æ”¶å›çš„ä¸€ä¸ªåœºæ™¯ï¼Œæˆ‘ä»¬æœ‰ä»¥ä¸‹æ•°æ®

$$x=[employed,revenue,student,\ldots,age]^{T}$$

$$X=[\vec{x}_{1},\vec{x}_{2},\vec{x}_{3},\ldots\vec{x}_{n}]^{T}$$

$$Y=[\vec{y}_{1},\vec{y}_{2},\vec{y}_{3},\ldots\vec{y}_{n}]^{T}$$

é€šè¿‡è®­ç»ƒä¸€æ£µæ ‘ï¼Œæ¥é¢„æµ‹ç”¨æˆ·è´·æ¬¾æ˜¯å¦èƒ½è¿˜æ¬¾ï¼Œå³ä½¿ $f(\vec{x})$ å°½é‡é€¼è¿‘ $Y$ ã€‚

![Decision Tree](/image/machine-learning/decision-tree.png)

ä¸çº¿æ€§æ¨¡å‹ä¸åŒçš„ç‚¹åœ¨äºï¼Œæ¯ä¸€ä¸ªèŠ‚ç‚¹å¦‚ä½•æ„å»ºä¸‹ä¸€ä¸ªèŠ‚ç‚¹è·Ÿçº¿æ€§æ¨¡å‹è°ƒæƒé‡æœ‰éå¸¸å¤§çš„ä¸åŒã€‚


## ç†µå’Œä¿¡æ¯å¢ç›Š

ç†µæ˜¯ç”¨æ¥æè¿°ç³»ç»Ÿçš„æ··ä¹±ç¨‹åº¦ï¼Œå³ä¸ç¡®å®šæ€§ã€‚åœ¨æœºå™¨å­¦æœŸä¸­åˆ†ç±»ä¸­è¯´ï¼Œç†µè¶Šå¤§å³è¿™ä¸ªç±»åˆ«çš„ä¸ç¡®å®šæ€§æ›´å¤§ï¼Œåä¹‹è¶Šå°ã€‚å½“ p=0 æˆ– p=1 æ—¶ï¼ŒH(p)=0,éšæœºå˜é‡å®Œå…¨æ²¡æœ‰ä¸ç¡®å®šæ€§ï¼Œå½“p=0.5æ—¶ï¼ŒH(p)=1,æ­¤æ—¶éšæœºå˜é‡çš„ä¸ç¡®å®šæ€§æœ€å¤§

![entropy](/image/machine-learning/entropy.png)

$$H(X)=\sum_{i=1}^{n}p(x_{i})\:I(x_{i})=-\sum_{i=1}^{n}p(x_{i})\log_{2}p(x_{i})$$

### å“ˆå¼—æ›¼ç¼–ç 

å‡è®¾æˆ‘ä»¬çš„å“ˆå¼—æ›¼ç¼–ç åªæœ‰å››ä¸ªå­—æ¯ï¼ŒA(00), B(01), C(10), D(11)ï¼Œå‡è®¾ï¼Œæ¯ä¸ªå­—æ¯çš„å‡ºç°ç‡æ˜¯ P(A)= 50%  P(B)= 25%  P(C)= 12.5%  P(D)= 12.5%ï¼Œåˆ™è¿™ä¸ªç³»ç»Ÿçš„ç†µä¸º H = 1.75ï¼Œè€Œæˆ‘ä»¬ç”¨äº† 2bit æ¥è¡¨ç¤ºã€‚

ä½†æ˜¯å¦‚æœæˆ‘ä»¬çš„ç¼–ç ç”¨çš„æ˜¯ A(0), B(10), C(110), D(111)ï¼Œé‚£ä»–çš„å¹³å‡ bit = 1 * 0.5 + 2 * 0.25 + 3 * 0.125 + 3 * 0.125 = 1.75 bit

è¿™é‡Œä¾‹å­è¡¨ç¤ºäº†ï¼Œå¦‚æœç³»ç»Ÿçš„ç†µè¶Šä½ï¼Œå®é™…ä¸Šç¼–ç æ•°è¶Šä½ã€‚

### ä¿¡æ¯å¢ç›Š

é€šè¿‡åˆšæ‰çš„ä¾‹å­æˆ‘ä»¬éœ€è¦åšçš„æ˜¯ï¼Œé€šè¿‡ä¸æ–­åœ°å¢åŠ èŠ‚ç‚¹ï¼Œè®©å­èŠ‚ç‚¹çš„ç†µé™ä½ï¼Œä»è€Œè¡¨ç¤ºè¿™ä¸ªæ ‘ä¿¡æ¯è¶Šå¤šã€‚ -->

# MLP
åˆšæ‰æ‰€è®²çš„çº¿æ€§æ¨¡å‹ï¼Œå¦‚æœç”¨å›¾ç”»å‡ºæ¥å°±æ˜¯è¿™æ ·

![Linear model](/image/machine-learning/linear-model.png)

æ•°å­¦å…¬å¼æ˜¯ä¸‹é¢è¿™ä¸ª:
$$f(\vec{x})=\sum_{i=1}^{n}w_{i}x_{i}+b$$

å½“æˆ‘ä»¬æŠŠå¾ˆå¤šçº¿æ€§æ¨¡å‹ï¼Œæ·»åŠ æ¿€æ´»å‡½æ•°ï¼Œç»„åˆåœ¨ä¸€èµ·ï¼Œå°±æˆäº†ä¸€ä¸ªå¤šå±‚æ„ŸçŸ¥æœºï¼Œä¹Ÿå°±æ˜¯MLP

![MLP](/image/machine-learning/MLP.png)


æ•°å­¦å…¬å¼æ˜¯ä¸‹é¢è¿™ä¸ª:

$$f(\vec{x})=\sum_{j=1}^{n}activation_{j}(\sum_{i=1}^{n}w_{i}x_{i}+b)$$

å…¶ä¸­çš„activationå°±æ˜¯æˆ‘ä»¬çš„æ¿€æ´»å‡½æ•°ï¼Œå¯ä»¥ä½¿RELUï¼Œæˆ–è€…sigmoidç­‰ç­‰ï¼Œå¦‚æœæ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œé‚£ä¹ˆMLPå°±æ˜¯ä¸€ä¸ªè¶…å¤§å‹çš„çº¿æ€§å‡½æ•°è€Œå·²ï¼Œæ­£æ˜¯å› ä¸ºæ¿€æ´»å‡½æ•°ï¼Œæ‰èƒ½è®©ä»–æ‹¥æœ‰é­”æ³•ã€‚

æ¨èä¸€ä¸ªğŸŒ°ï¼Œ[è¯†åˆ«æ‰‹å†™æ•°å­—](https://www.bilibili.com/video/BV1bx411M7Zx/?spm_id_from=333.1387.collection.video_card.click&vd_source=1c9c4de6b1ffda02913fc889a72af206)ã€‚

## å»ºç«‹æ•°å­¦æ¨¡å‹
å‚è€ƒè§†é¢‘ï¼Œæˆ‘ä»¬è¦å°†è¿™ä¸ªæ¨¡å‹å…ˆè½¬åŒ–ä¸ºæ•°å­¦é—®é¢˜ï¼Œä»–æœ‰4å±‚èŠ‚ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ä»å·¦åˆ°å³è®¾ä¸º$\overrightarrow{y_{0}}$,$\overrightarrow{y_{1}}$,$\overrightarrow{y_{2}}$,$\overrightarrow{y_{3}}$

æˆ‘ä»¬é€‰ç”¨Sigmoidä½œä¸ºæ¿€æ´»å‡½æ•°ï¼Œä»–çš„æ•°å­¦æè¿°ä¸ºï¼š

$$\begin{cases}
\vec{y}_3 = Sigmoid \left( \vec{w}_2 \cdot \vec{y}_2 + \vec{b}_2 \right) \\
\vec{y}_2 = Sigmoid \left( \vec{w}_1 \cdot \vec{y}_1 + \vec{b}_1 \right) \\
\vec{y}_1 = Sigmoid \left( \vec{w}_0 \cdot \vec{y}_0 + \vec{b}_0 \right) & 
\end{cases}$$

æ¥ä¸‹æ¥æˆ‘ä»¬éœ€è¦ä¼˜åŒ–è¿™ä¸ªæ•°å­¦æ¨¡å‹ï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦ä¸€ä¸ªæŸå¤±å‡½æ•°

$$cost = (\overrightarrow{y}_3 - \overrightarrow{y})^2$$

æˆ‘ä»¬è¦é€šè¿‡è°ƒæ•´$\overrightarrow{w}/\overrightarrow{b}$æ¥ä½¿æœ€ç»ˆçš„æŸå¤±å‡½æ•°è¾¾åˆ°æœ€å°

æˆ‘ä»¬å…ˆå›åˆ°çº¿æ€§æ¨¡å‹ä¸­ï¼Œå¦‚æœåªæœ‰ä¸€ä¸ªç®€å•çš„çº¿æ€§æ–¹ç¨‹$y=Sigmoid(w\cdot x + b)$ï¼Œé‚£æˆ‘ä»¬å…¶å®åªç”¨å¯¹wå’Œbæ±‚åå¯¼ï¼Œä½†æ˜¯æˆ‘ä»¬è¿™é‡Œæœ‰$\vec{w}_2$,$\vec{w}_1$,$\vec{w}_0$,$\vec{b}_2$,$\vec{b}_1$,$\vec{b}_0$å…­ä¸ªå˜é‡ï¼Œè¿™ä¸ªæ—¶å€™å°±è¦ç”¨åˆ°åå‘ä¼ æ’­

## åå‘ä¼ æ’­
åå‘ä¼ æ’­å®é™…ä¸Šå°±æ˜¯åŸºäºé“¾å¼æ³•åˆ™ï¼Œåœ¨åˆšåœ¨çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä»¥æœ€ç»ˆæŸå¤±å‡½ä¸ºä¾‹ï¼Œå°†å…¶å¯æ‹†åˆ†ä¸º

$$\begin{cases}
C=\frac{1}{N}\sum_{i=1}^{N}\vec{v} \\
\vec{v} = (\overrightarrow{y}_3 - \overrightarrow{y})^2 \\
\vec{y}_3 = Sigmoid(\vec{a}_3) \\
\vec{a}_3 = (\vec{w}_2 \cdot \vec{y}_2 + \vec{b}_2) \\
...
\end{cases}$$

å½“æˆ‘ä»¬æ±‚å‡º$[\frac{\partial C}{\partial \vec{w}_{0}},\frac{\partial C}{\partial \vec{w}_{1}},\frac{\partial C}{\partial \vec{w}_{2}},\frac{\partial C}{\partial \vec{b}_{0}},\frac{\partial C}{\partial \vec{b}_{1}},\frac{\partial C}{\partial \vec{b}_{2}}]$æ¢¯åº¦åï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥ä¸æ–­åœ°è®­ç»ƒ