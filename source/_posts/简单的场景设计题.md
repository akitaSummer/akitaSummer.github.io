---
title: 简单的场景设计题
date: 2023-06-13 11:02:33
tags:
  - 场景设计
categories: 学习笔记
---

# 前言

平常收集到的场景设计题，比较杂，仅作为笔记使用

# 短网址系统设计

## 题目

1. 分为两个接口

- 从一个长网址生成一个短网址。需要考虑：同一个长网址，多次创建的短网址是否相同
- 用户访问短网址时，需要能跳回到原始的长网址

2. 需要考虑跨机房部署问题
3. 考虑跨海域全球部署问题
4. 考虑统计某个域名下的 URL/host 访问 uv 和 pv

## 答案

1. 一开始需要能考虑系统承载容量，例如：

- 每天 100 亿访问量
- 每天生成 1000w 条短网址记录

2. 然后考虑短网址的生成算法，方案有很多种

- 最简单实现：自增 id 实现，这个不可逆，同一个长网址会生成多个短网址
- hash+序号冲突
- 使用 kv 存储双向对应关系，可逆，但存储用量比较大

3. 302 跳转问题，附带可以讨论网址访问量计数问题
4. 需要考虑跨机房部署问题
5. 考虑跨海域全球部署问题
6. 能给出合理的统计需求，例如用 hadoop 做 MR

# 计数系统设计

## 题目

希望能够有一个统一的计数服务来满足该需求，主要功能要求根据业务需求查询指定的文章的计数统计（播放数，阅读数，评论数等），要求：支持实时更新各种计数，支持高并发查询，需要给出系统存储设计方案，对外输出接口设计；

## 答案

1. 方案一：直接使用数据库+cache 的方案，方案简单有效，数据量大之后采用分库方案，扩展性有限，但开发和运维成本低，性能方面通过 cache 优化，存在热点数据（可能导致 cache 经常被清理）；

2. 方案二：采用 redis 作为 kv 存储，查询效率足够高，但需要考虑资源使用问题，假设有 10 亿的文章，帖子和评论，如何保证以更低的成本满足该需求；主要问题需要较多资源，成本较高，如何设计更好数据结构；

3. 方案三：可以自己开发 counter 模块，需要考虑 kv 存储方案，value 的设计，保证使用更少内存；还需要考虑的点：容灾备份机制；数据扩展问题；

4. 方案四：可能业务方经常新增计数需求，需要考虑 counter 服务的列扩展性，故设计的数据结构需要考虑列扩展问题；

5. 其他：业务写入 QPS 可能比较高，考虑写入压力问题，数据写入去重问题，即同一个用户转发之类操作需要幂等性（一定程度保证即可）

# 直播消息服务设计

消息服务是直播产品中一个非常核心的服务。直播间内的聊天、弹幕、礼物等消息，都是通过消息服务发送。

## 题目
设计一个消息服务，支持直播间内的用户进行聊天、发弹幕等操作。
- 衡量一个消息服务的核心指标有哪些？
- 基于候选者的方案，如何监控和优化这些核心指标？
- （深入）拉模型的分布式方案

## 答案
1. 方案设计
- 主流的设计主要有两种：推模型和拉模型。（任何一种都是合适的设计） 推模型简述： 基于长连接，直播间内的观众发的消息，异步通过长连接发送给房间内的其他观众。候选人的架构图应该没有硬伤。 深入的问题（言之有理即可）：
- 长连接的架构（协议、鉴权、扩容、重连、到达率等）
- 消息放大问题如何解决（比如一个有 1 万人的房间，任何一个人发的消息，都会产生 1 万个消息，相当于放大了 1 万倍） 消息聚合、消息多级推送（类似 CDN 的方式）
- 直播间用户列表怎么存储和优化 推送消息时，需要房间内所有用户消息，对于观众比较多的房间，需要考虑数据分片、本地缓存等手段进行优化。 拉模型简述： 拉模型类型类似于一个消息队列的模型。每个房间会有一个消息列表，直播间内用户发的消息会放在相应的消息列表中。直播间内的观众通过前端轮询拉消息接口， 把消息拉到客户端展示。 深入的问题（言之有理即可）：
- 房间的消息如何存储（由于消息有时效性，所以只需要存储最近一段时间的数据）
- 轮询方式如何优化
- 拉接口如何优化（local cache 等）
2. 核心指标
- 消息每秒吞吐量、消息到达率、消息延时（像稳定性这种，属于通用的基本指标）。
- 核心的优化方式（提供一些方式，其它的只要合理即可）：
3. 监控方式：
- 吞吐量（类似于 qps，打 metrics 等都可以）
- 到达率：对于推模型，基本等价于长连接的到达率监控；对于拉模型，性价比较高的是只监控主播的（因为只有主播是全程在直播间的）
- 延时：需要关注手机和服务端的时间不一致的问题
4. 优化：
- 吞吐量：批量发送、多级推送等
- 到达率：一般推模型需要重点关注，主要是对于长连接的到达率优化，包含死连接检测等。
- 延时：一般拉模型需要重点关注，对于每次都拉到消息的房间，减少轮询间隔和长连接拉模式等。
5. 拉模型的分布式方案
- 类似于一个分布式存储系统，解决水平和垂直扩展的问题。

# 秒杀系统设计

## 题目

“米粉节”3000 万人抢购，峰值 QPS100 万，请设计抢购系统的技术架构，保证可靠性并且不能超卖。
设计要点： 
1 可以支撑 QPS 100 万，能分析系统的瓶颈 
2 如何尽快把商品卖完，但不能超卖 
3 考察系统扩展性、容灾能力

## 答案

设计思路：限流+策略+反作弊，一般情况下候选人想不到完善的设计，我们主要看其设计能否满足前面的需求。
1 可以满足业务需求，抗住压力，不会超卖
2 能考虑到到各种情况下的灾备方案，系统易于对商品种类，抢购规模进行扩展，

# 海量评论系统

## 题目

如何设计海量评论存储系统，支持快速大量的写入及任意翻页的需求。 
- 主要是看候选人对“复杂”业务的分解，在实际存储时是否会通过类似二级索引的方式来优化翻页效率；
- 在业务上冷热数据的不同缓存处理；是否会考虑使用缓冲区避免突发流量的写入；
- 偶尔会关联问到全局唯一 ID 的设计方案。 
- 如果获取评论列表的时候，支持按照一定热度对评论进行排序，又该如何进行设计

## 答案

评论大量写入优化方案：
1. id 分配策略，写入量太多如何进行扩展（自己实现分片策略，或者采用分布式存储系统，如果选用分布式系统介绍选型方案和实现细节）；
2. 查询方面如果按照时间逆序，索引可以采用 zset 方式，但需要考虑单个文章评论索引太多问题，故可以采用自建索引或者利用 redis 等拆分索引大小；其次需要考虑评论被删除后，索引中大片数据无法使用情况，采用重建索引还是单独提供删除索引？
3. 头条环境下对评论做了个性化排序，索引的机制如何进行调整；（加分项目）
4. 容错机制考虑；（加分项目）

# 推送去重系统设计

## 题目

推送系统去重模块设计：头条有几亿的活跃设备数，运营日常推送时，会针对同一篇新闻进行多维度推送，比如先推送一次北京用户，再补推一次时政用户，再推一次全国用户。同一个用户可能会被匹配到多次，要如何避免用户收到重复的推送。

## 答案

1. 去重必定会有状态，所以该系统设计的核心在于“状态”数据的存储，特点就是数据量大，去重要求精确。
2. 常规的思路是用分布式 KV 存储（Redis、Memcache 等），去重模块本身无状态。基本合格。
3. 考虑本地 Redis 或 Memcache 方案，去重模块按用户分片。
4. 能根据状态数据的特性，考虑自己实现存储，并且实现半持久化（程序重启数据不丢），内存+磁盘存储可满足需求，根据面试者存储数据的选择和持久方式的解答。
5. 基于共享内存（或类似的磁盘文件映射 mmap 等方式），并且能较好的给出基于块存储实现 Hash 的表，以及 Hash 表的增加和查询。因为数据量大，存储需要考虑很多优化方案，基于 bit 的操作等。
6. 在 4 的基础上，考虑到数据数据清除（以天为单位清除）。考虑 Android 设备可以自己掌握弹窗的，在端上做保底去重。

# 类似微博 timeline 功能实现，考虑怎么设计支持「热门」

## 题目

设计类似微博 timeline 功能，好友发表了最新的微博之后，能够在自己的 feed 流中按照时间顺序获取到最新的微博，也能通过加载更多功能获取之前历史中的微博数据。 如果考虑支持按照热度对好友微博进行排序展现，热度主要根据用户阅读，评论和点赞数据评估，该系统该如何扩展。

## 答案

1. 写入系统：设计写入服务，帖子 id 分配策略（采用全局分配器），如何解决 qps 突增等情况，写入 qps 较高时，通过分片等方式提升写入效率，当然目前也可以采用写入效率较高的分布式存储系统；
2. 索引构建：由于要支持 timeline 或者其他的排序方式，故需要单独的排序策略，这里单独针对 timeline 排序详细说明，timeline 按照发表者时间顺序，一般有 3 中处理方案：
- 拉的方式，按照发布者建立时间索引，需要考虑关注人数较多时，获取索引效率问题；
- 推模式：发布的时通过异步消息队列，将消息投递到各个阅读者，此处主要问题推的效率和存储空间的开销；
- 混合模式：粉丝较多用户采用拉模式，在线用户直接投递等提高查询效率；
2 中需要考虑索引数据结构设计，排序 cursor 值选取问题；
如果是热点排序：全局热点比较简单，可以直接维护一个热度计算模块，接入热度事件计算流，如果事件较多，则可以使用分布式实时计算框架提高计算效率，对于热度超过一定阈值的进入到热度排序系统中，按照从高到底排序即可（热度排序系统按照一定量截断多余数据）；如果热度支持不同时间段分别组织；其次如果按照话题拆分呢？又如何设置热度排序系统？

# 分布式 cache 系统设计

## 题目

工业环境中对数据查询访问效率比较大，但数据量有比较大，单机 cache 无法满足业务需求，故需要实现一个分布式 cache 系统，满足业务查询效率和数据量要求，请设计这样一个系统

## 答案

1. 基础数据结构采用 hash 或者 map 的方式均可以；
2. 考虑线程安全性，同时兼顾查询效率，是否考虑分桶等机制
43. 分布式环境下：主要考察分片方案，如何做到使用者透明，怎么解决某一个节点故障问题，雪崩问题；

# 12306 系统售票系统设计

## 题目

设计 12306 系统的售票模型，实现查询余票和购票的功能。在春运中采用什么样的架构支撑。

## 答案

业务模型：
有多种解法，每种有各自优缺点：
1 记录任意两个车站的余票数量，需要维护 Cn2 共 105 个车站对的余票信息。该方案查询效率非常高，更新时有放大效应，需要更新多个车站对的余票信息。
2 记录每一张席位的售卖记录，查询时遍历每个席位，算出余票信息。该方案更新效率较高，查询时需要遍历。
3 将全程分解为原子区间，记录每个区间的可用车票数目。购买时只要起始站和终点站之间的原子区间有票，那可以出一张票。该方案查询和更新效率都很高，但不能保证买到的是同一个席位。更适合用来模糊查询。
在不考虑性能的情况下，方案可以满足查询和购买的基本需求，
性能上满足要求，并给出准确的分析
架构：
1 技术选型可以实现需求，对技术栈性能和特性都比较清楚
2 能够对系统瓶颈做出分析，设计比较系统的容灾方案

# 分布式定时任务管理

## 题目

设计一个分布式定时任务管理系统，按照业务需求，解决任务管理和资源分配问题

## 答案

1. 任务提交管理
2. 定时器；
3. 分布式的任务调度；
4. 失败容错机制

# 多人协同文档应用项目

## 题目

设计一个多人协作（可同时修改）的文档应用，尽量满足以下需求： 
- 多个人可以同时编辑同一份在线文档，尽量做到一个人编辑时不会影响其他人的编辑；
- 文档的修改应尽可能实时地反映在编辑者的界面上，例如 A 增加了一行，那么 B 应该能在较短时间内看到这个变更；
- 尽可能减少客户端与后端的数据传输量，并尽可能提高后端处理时的响应速度； 4.编辑的内容应可能做到不丢失，即自动保存功能
请大致说说： 如果让你设计，那么后端架构、存储是怎么样的？客户端的产品逻辑是怎么样的（什么时候保存，有编辑时往服务端发送什么内容，如何获取到对方的修改内容等）

## 答案

1. 能够想到类似 diff 工具，出现同一行冲突时由用户来解决，这样能避免自动合并有可能出错等情况。
2. 采用长连接方式传递修改的 diff， 服务端转换之后传递 diff 到客户端。
3. 针对统一文档的的所有用户 hash 到同一台机器。
4. 能够采用 Myer’s diff / Operational Transformation 等算法。
5. 传递 diff， 如果有多机处理统一文档的需求，需要保证操作的一致性。

# 文件分发系统设计

## 题目

用户不断把文件上传到服务器上 需要设计一个文件分发系统，把这些文件分发到服务器集群上（规模 100-10000 级别）
要求：速度更快，可靠性更高

## 答案

1 最基本解法，需要考虑分级分发，且有基本的容错性考虑（重试，校验之类的，针对网络故障服务器宕机等常见场景的策略）
2 可考虑 p2p 的方式，进一步的可通过优化 p2p 分发策略来提升效率
3 考虑系统单点消除，横向扩展，可维护性提升，监控，追查问题效率等要素

# 客户端配置系统设计
## 题目
头条 app 客户端做了比较多 feature，为了对这些功能做评估或者动态控制，需要支持云控 ab 测试方案，故需要设计一个在线动态配置管理系统设计，该系统要求能比较便捷进行参数动态修改和 ab 测试，请设计该系统满足该需求，需要考虑动态配置项比较多和易用性。
## 答案
1. 能清楚的画出整个架构图，基于 MySQL 做存储，有可视化操作后台直接操作 MySQL，API 层解析 MySQL 数据下发数据。
2. API 层请求量会很大，考虑到请求 MySQL 时需要做缓存并给合理的缓存方案（本地缓存即可）。
3. 配置需求各式各样，如何抽象配置是该系统的关键。和 Lua 或 Python 等脚本语言来描述配置比较合适。
